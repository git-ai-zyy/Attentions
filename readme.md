In this repository, I will implement the different attention mechanisms in PyTorch. The attention mechanisms are implemented in the `attention` file. Most of the code using [Pytorch](https://pytorch.org/) and [Triton](https://triton-lang.org/main/index.html).

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

---

## Multi Headed Attention
